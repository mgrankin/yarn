{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "from model_loader import *\n",
    "from types import SimpleNamespace\n",
    "from fastapi import FastAPI, Request\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "model_name = '/models/Yarn-Llama-2-7b-128k'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, model_max_length=sys.maxsize, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "from scaled_rope.modeling_llama_together_yarn import LlamaForCausalLM\n",
    "from scaled_rope.configuration_llama import LlamaConfig\n",
    "model_cls = LlamaForCausalLM\n",
    "config_cls = LlamaConfig\n",
    "\n",
    "\n",
    "config = config_cls.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = model_cls.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    config=config,\n",
    "    quantization_config=None\n",
    ")\n",
    "\n",
    "app = FastAPI(title=f\"Serving {model_name}\", version=\"0.1\",)\n",
    "\n",
    "@app.post(\"/get_n_token/\")\n",
    "def get_n_token(prompt:str = Field(\"You say you're Leo Tolstoy, but in reality\", title='Model prompt')):\n",
    "    return {\"n_token\":  len(tokenizer.tokenize(prompt))}\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    prompt:str = Field(\"You say you're Leo Tolstoy, but in reality\", title='Model prompt')\n",
    "    max_new_tokens:int = Field(256, ge=1, le=128000, title='Number of tokens generated in each sample')\n",
    "    temperature:float = Field(1.0, ge=0.1, le=10.0, title='Temperature parameter for generation')\n",
    "    top_k:int = Field(40, ge=1, le=30000)\n",
    "    repetition_penalty:float = Field(1.1, ge=1.0, )\n",
    "    penalty_alpha:float = Field(0.0, ge=0.0, )\n",
    "    num_return_sequences:int = Field(1, ge=1, le=5, title='Number of samples generated')\n",
    "\n",
    "@app.post(\"/generate/\")\n",
    "def gen_sample(prompt: Prompt):\n",
    "        \n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, pad_token_id=tokenizer.eos_token_id,\n",
    "                    temperature=prompt.temperature, repetition_penalty=prompt.repetition_penalty,\n",
    "                    top_k=prompt.top_k, penalty_alpha=prompt.penalty_alpha, do_sample=prompt.temperature is not None)\n",
    "    input_tokens = len(tokenizer.tokenize(prompt.prompt))\n",
    "    if input_tokens + prompt.max_new_tokens > config.max_position_embeddings: \n",
    "        return {\"error\": f'N of input tokens ({input_tokens}) + prompt.max_new_tokens ({max_new_tokens}) > config.max_position_embeddings ({max_position_embeddings})'}\n",
    "\n",
    "    return {\"replies\": pipe(prompt.prompt, num_return_sequences=1, max_new_tokens=prompt.max_new_tokens)[\n",
    "            0][\"generated_text\"][len(prompt.prompt):]}\n",
    "\n",
    "@app.get(\"/health\")\n",
    "def healthcheck():\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app/scaled_rope/modeling_llama_together_yarn.py:523: UserWarning: operator() profile_node %34 : int[] = prim::profile_ivalue(%32)\n",
      " does not have profile information (Triggered internally at /opt/pytorch/pytorch/third_party/nvfuser/csrc/graph_fuser.cpp:104.)\n",
      "  kv = repeat_kv(kv, self.num_key_value_groups)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'replies': '\\nyou\\'re an old gentleman of seventy-six. A very respectable old man--a\\nrespectable author also....\"[18]\\n\\nWith these words he turned to go. The \"old man of seventy-six\" saw him\\noff with the remark:\\n\\n\"There is something good about your work, my young friend! It will come to\\nsomething.\"\\n\\nIt was said for no particular reason; still it may have been true.\\n\\nBut the best that could be said of all this story from the point of view\\nof Tolstoy\\'s literary reputation at the time was that the critics were\\nmore indulgent than unfriendly. They made some allowance for his youth,\\nand they forgave many of his faults because they attributed them to a\\nnatural excess of zeal on behalf of the cause of enlightenment.\\n\\nThe general verdict is expressed by a critic who wrote thus about _Poor\\nLiza_:[19]\\n\\n\"We are inclined to excuse the novelist for not being able to do more\\nthan he has done. He began too soon, and he did much in a year which\\nwould not'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from types import SimpleNamespace\n",
    "\n",
    "prompt = SimpleNamespace()\n",
    "prompt.prompt = \"You say you're Leo Tolstoy, but in reality\"\n",
    "prompt.max_new_tokens = 256 \n",
    "prompt.temperature = 1.0\n",
    "prompt.top_k = 40 \n",
    "prompt.repetition_penalty = 1.1\n",
    "prompt.penalty_alpha = 0.0\n",
    "prompt.num_return_sequences = 1\n",
    "\n",
    "gen_sample(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash Attention installed\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from flash_attn.flash_attn_interface import (\n",
    "        flash_attn_func, \n",
    "        flash_attn_kvpacked_func, \n",
    "        flash_attn_qkvpacked_func,\n",
    "        flash_attn_varlen_kvpacked_func, \n",
    "    )\n",
    "    from flash_attn.bert_padding import unpad_input, pad_input\n",
    "    flash_attn_v2_installed = True\n",
    "    print('>>>> Flash Attention installed')\n",
    "except ImportError:\n",
    "    flash_attn_v2_installed = False\n",
    "    raise ImportError('Please install Flash Attention: `pip install flash-attn --no-build-isolation`')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Flash RoPE installed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    from flash_attn.layers.rotary import apply_rotary_emb_func\n",
    "    flash_rope_installed = True\n",
    "    print('>>>> Flash RoPE installed')\n",
    "except ImportError:\n",
    "    flash_rope_installed = False\n",
    "    raise ImportError('Please install RoPE kernels: `pip install git+https://github.com/HazyResearch/flash-attention.git#subdirectory=csrc/rotary`')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
